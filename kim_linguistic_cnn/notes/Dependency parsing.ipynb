{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_jar = '../stanford-corenlp-full-2016-10-31/stanford-corenlp-3.7.0.jar'\n",
    "path_to_models_jar = '../stanford-corenlp-full-2016-10-31/stanford-corenlp-3.7.0-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "r = re.compile(r'\\d{3,100}[ -]+\\d{3,100}[ -]+\\d{3,100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dependency_parse(sentence):\n",
    "    try:\n",
    "        result = dependency_parser.raw_parse(sentence)\n",
    "        dep = result.__next__()\n",
    "        return list(dep.triples())\n",
    "    except:\n",
    "        print(\"EXCEPTION: while parsing sentence:\")\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ARFTS', 'NNS'), 'amod', ('binds', 'JJ')),\n",
       " (('binds', 'JJ'), 'advmod', ('specially', 'RB')),\n",
       " (('ARFTS', 'NNS'), 'nmod', ('domain', 'NN')),\n",
       " (('domain', 'NN'), 'case', ('to', 'TO')),\n",
       " (('domain', 'NN'), 'det', ('a', 'DT')),\n",
       " (('domain', 'NN'), 'amod', ('distinct', 'JJ')),\n",
       " (('domain', 'NN'), 'nmod', ('XIAP-BIR3', 'NN')),\n",
       " (('XIAP-BIR3', 'NN'), 'case', ('in', 'IN'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_parse(\"ARFTS specially binds to a distinct domain in XIAP-BIR3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output is wrong. You need to fix it! Compare the output from the Stanford CoreNLP demo\n",
    "\n",
    "__Caution:__ Do not use the parser from NLTK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adopted from https://github.com/smilli/py-corenlp\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "properties={'annotators': 'depparse', 'outputFormat': 'json'}\n",
    "\n",
    "def parse(fname):\n",
    "    save_output = ''\n",
    "    with open(fname, 'r') as fhandle, open(fname + '.deps.json', 'w') as whandle:\n",
    "        for line in fhandle:\n",
    "            output = nlp.annotate(line.strip(), properties)\n",
    "            whandle.write(json.dumps(output['sentences'][0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the dependencies on a sample file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Royal likes Mangoes\r\n",
      "Royal wants to go back to Mangalore\r\n",
      "Royal wants to spend the rest of his life reading books and learning new languages.\r\n",
      "at times , the suspense is palpable , but by the end there 's a sense that the crux of the mystery hinges on a technicality that strains credulity and leaves the viewer haunted by the waste of potential ."
     ]
    }
   ],
   "source": [
    "%cat ../sampleFile.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parse('../sampleFile.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Details\n",
    "The above way of parsing gives three levels of dependency parsing:\n",
    "- basicDependencies,\n",
    "- enhancedDepencies, and\n",
    "- enhancedPlusDepencies\n",
    "\n",
    "For now, I am saving all the information that I get from dependency parsing. I will later exploe what each of these levels mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (40,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-277-12841b831d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mheadword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheadtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dep_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0munfold_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-277-12841b831d27>\u001b[0m in \u001b[0;36munfold_tags\u001b[0;34m(tags)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mdep_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mpos_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../sampleFile.txt.deps.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-277-12841b831d27>\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(dep_tag, pos_tag)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mpos_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_narray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mdep_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_dependency_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdep_narray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mpos_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_narray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (40,) (3,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "basic_dep_tags = ['csubj', 'aux', 'acl:relcl', 'mark', 'expl', 'amod', 'acl', 'parataxis', 'compound',\n",
    "            'advmod', 'nmod:poss', 'cc:preconj', 'det', 'case', 'ROOT', 'punct', 'nmod:npmod', \n",
    "            'nsubjpass', 'det:predet', 'advcl', 'root', 'dep', 'mwe', 'xcomp', 'nmod', 'cop', \n",
    "            'cc', 'nsubj', 'csubjpass', 'appos', 'conj', 'nummod', 'discourse', 'auxpass', 'ccomp',\n",
    "            'nmod:tmod', 'iobj', 'compound:prt', 'dobj', 'neg', 'NO_DEP']\n",
    "\n",
    "pos_tags = ['RBS', \"''\", 'VB', '#', '.', 'WP$', 'SYM', 'LS', 'WDT', 'NNP', 'TO', 'CD', 'NNPS', \n",
    "            'NN', 'MD', 'RBR', 'JJS', 'VBN', 'VBP', '``', 'WRB', 'JJR', 'VBD', 'FW', 'RB', 'NNS',\n",
    "            'POS', ',', 'PDT', 'UH', 'VBG', '$', 'PRP$', 'VBZ', 'PRP', ':', 'WP', 'IN', 'CC', 'DT',\n",
    "            'JJ', 'RP', 'EX', 'NO_POS']\n",
    "\n",
    "def get_dep_pos(string):\n",
    "    this_pos = []\n",
    "    json_dict = json.loads(string)\n",
    "\n",
    "    for token in json_dict['tokens']:\n",
    "        this_pos.append(pos_tags.index(token['pos']))\n",
    "\n",
    "    len_dep = len(this_pos)\n",
    "    this_dep = [None] * len_dep\n",
    "    headwords = [None] * len_dep\n",
    "    this_head_index = [None] * len_dep\n",
    "\n",
    "    for dep in json_dict['basicDependencies']:\n",
    "        index, dependency = dep['dependent'], dep['dep']\n",
    "        this_dep[index - 1] = dep_tags.index(dependency)\n",
    "        headwords[index - 1] = dep['governorGloss']\n",
    "        this_head_index[index - 1] = dep['governor']\n",
    "\n",
    "    this_head_dep = []\n",
    "    this_head_pos = []\n",
    "\n",
    "    for head_index in this_head_index:\n",
    "        if head_index - 1 < 0:\n",
    "            this_head_dep.append(basic_dep_tags.index('NO_DEP'))\n",
    "            this_head_pos.append(pos_tags.index('NO_POS'))\n",
    "        else:\n",
    "            this_head_dep.append(this_dep[head_index - 1])\n",
    "            this_head_pos.append(this_pos[head_index - 1])\n",
    "\n",
    "    tags = this_dep + this_pos\n",
    "    headtags = this_head_dep + this_head_pos\n",
    "    return headwords, headtags, tags\n",
    "\n",
    "def one_hot(dep_tag, pos_tag):\n",
    "    basic_dependency_size = 40\n",
    "    pos_size = 43\n",
    "    dep_narray = np.array(dep_tag)\n",
    "    pos_narray = np.array(pos_tag)\n",
    "    \n",
    "    dep_one_hot = np.zeros((dep_narray.size, basic_dependency_size + 1))\n",
    "    pos_one_hot = np.zeros((pos_narray.size, pos_size + 1))\n",
    "    \n",
    "    dep_one_hot[np.arange(basic_dependency_size), dep_narray] = 1\n",
    "    pos_one_hot[np.arange(len(pos_tag)), pos_narray] = 1\n",
    "\n",
    "    for x, y in zip(dep_one_hot, pos_one_hot):\n",
    "        print(x.tolist() + y.tolist())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     dep_one_hot[np.arange(basic_dependency_size), dep_narray] = 1\n",
    "#     pos_one_hot[np.arange(pos_size), pos_narray] = 1\n",
    "\n",
    "#     concatenated_one_hot = []\n",
    "#     for x, y in zip(dep_one_hot, pos_one_hot):\n",
    "#         concatenated_one_hot.append(x.tolist() + y.tolist())\n",
    "\n",
    "#     return concatenated_one_hot\n",
    "              \n",
    "def unfold_tags(tags):\n",
    "    mid = len(tags) // 2 \n",
    "    dep_tag = tags[:mid]\n",
    "    pos_tag = tags[mid:]\n",
    "    return(one_hot(dep_tag, pos_tag))\n",
    "    \n",
    "with open('../sampleFile.txt.deps.json') as json_file:\n",
    "    for line in json_file:\n",
    "        headword, headtag, wordtag = get_dep_pos(line)\n",
    "        unfold_tags(headtag)\n",
    "        break\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "all_deps_list = set()\n",
    "all_pos_list = set()\n",
    "\n",
    "for file in glob.glob('../data/*.json'):\n",
    "#     print(file)\n",
    "    with open(file) as json_file:\n",
    "        for line in json_file:\n",
    "            json_dict = json.loads(line)\n",
    "\n",
    "            for tok in json_dict['basicDependencies']:\n",
    "                if tok['dep'] not in all_deps_list:\n",
    "                    all_deps_list.add(tok['dep'])\n",
    "\n",
    "            for tok in json_dict['tokens']:\n",
    "                if tok['pos'] not in all_deps_list:\n",
    "                    all_pos_list.add(tok['pos'])\n",
    "\n",
    "print(len(all_deps_list))\n",
    "print(len(all_pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: sampleFile.txt.deps.json: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%cat sampleFile.txt.deps.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do:\n",
    "- readme instruction to run the server first\n",
    "- nsubj:xcomp, experiments retaining it and separating it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
